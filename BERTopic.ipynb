{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c64b2ae",
   "metadata": {},
   "source": [
    "# BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf848c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\p_uli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\p_uli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\p_uli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "# from cuml.cluster import HDBSCAN\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.preprocessing import normalize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup   \n",
    "import contractions,unicodedata\n",
    "\n",
    "\n",
    "nltk.download('stopwords')                              # Download Stopwords.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer                     # Stemmer\n",
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "643d75a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\split_threads.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c96ef53f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>email_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>allen-p\\all_documents\\1#1</td>\n",
       "      <td>December 14, 2000 - Bear Stearns' predictions ...</td>\n",
       "      <td>In today's Daily Update you'll find free repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>allen-p\\all_documents\\10#1</td>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>allen-p\\all_documents\\100#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>allen-p\\all_documents\\100#2</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>Phillip,\\n Below is the issues &amp; to do list as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>allen-p\\all_documents\\100#3</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     email_id  \\\n",
       "0           0    allen-p\\all_documents\\1#1   \n",
       "1           1   allen-p\\all_documents\\10#1   \n",
       "2           2  allen-p\\all_documents\\100#1   \n",
       "3           3  allen-p\\all_documents\\100#2   \n",
       "4           4  allen-p\\all_documents\\100#3   \n",
       "\n",
       "                                             subject  \\\n",
       "0  December 14, 2000 - Bear Stearns' predictions ...   \n",
       "1                       Bloomberg Power Lines Report   \n",
       "2        Consolidated positions: Issues & To Do list   \n",
       "3        Consolidated positions: Issues & To Do list   \n",
       "4        Consolidated positions: Issues & To Do list   \n",
       "\n",
       "                                               email  \n",
       "0  In today's Daily Update you'll find free repor...  \n",
       "1  Here is today's copy of Bloomberg Power Lines....  \n",
       "2  From our initial set of meetings with the trad...  \n",
       "3  Phillip,\\n Below is the issues & to do list as...  \n",
       "4                                                     "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b97c26f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(472970, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99630688",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b8a768e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing all empty emails\n",
    "data['email'] = data['email'].str.strip()\n",
    "data['subject'] = data['subject'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a4bd3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty values for NA\n",
    "data['email'].replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1e86dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop NAs\n",
    "data.dropna(subset=['email'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1563f640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443173, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6e3b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utl removal\n",
    "def remove_url(text):\n",
    "    text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    return text\n",
    "data['clean_text'] = data['email'].apply(lambda x: remove_url(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "42610ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html tag removal\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(lambda x: strip_html(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "268cd42c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>email_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>allen-p\\all_documents\\1#1</td>\n",
       "      <td>December 14, 2000 - Bear Stearns' predictions ...</td>\n",
       "      <td>In today's Daily Update you'll find free repor...</td>\n",
       "      <td>In today's Daily Update you will find free rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>allen-p\\all_documents\\10#1</td>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>allen-p\\all_documents\\100#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>allen-p\\all_documents\\100#2</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>Phillip,\\n Below is the issues &amp; to do list as...</td>\n",
       "      <td>Phillip,\\n Below is the issues &amp; to do list as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>allen-p\\all_documents\\101#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     email_id  \\\n",
       "0           0    allen-p\\all_documents\\1#1   \n",
       "1           1   allen-p\\all_documents\\10#1   \n",
       "2           2  allen-p\\all_documents\\100#1   \n",
       "3           3  allen-p\\all_documents\\100#2   \n",
       "5           5  allen-p\\all_documents\\101#1   \n",
       "\n",
       "                                             subject  \\\n",
       "0  December 14, 2000 - Bear Stearns' predictions ...   \n",
       "1                       Bloomberg Power Lines Report   \n",
       "2        Consolidated positions: Issues & To Do list   \n",
       "3        Consolidated positions: Issues & To Do list   \n",
       "5        Consolidated positions: Issues & To Do list   \n",
       "\n",
       "                                               email  \\\n",
       "0  In today's Daily Update you'll find free repor...   \n",
       "1  Here is today's copy of Bloomberg Power Lines....   \n",
       "2  From our initial set of meetings with the trad...   \n",
       "3  Phillip,\\n Below is the issues & to do list as...   \n",
       "5  From our initial set of meetings with the trad...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  In today's Daily Update you will find free rep...  \n",
       "1  Here is today's copy of Bloomberg Power Lines....  \n",
       "2  From our initial set of meetings with the trad...  \n",
       "3  Phillip,\\n Below is the issues & to do list as...  \n",
       "5  From our initial set of meetings with the trad...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# contractions removal\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(lambda x: replace_contractions(x))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "914cb6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing numbers\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    return text\n",
    "\n",
    "data['clean_text'] = data['clean_text'].apply(lambda x: remove_numbers(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c689aac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "data['clean_text'] = data.apply(lambda row: nltk.word_tokenize(row['clean_text']), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e43c8282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>email_id</th>\n",
       "      <th>subject</th>\n",
       "      <th>email</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>allen-p\\all_documents\\1#1</td>\n",
       "      <td>December 14, 2000 - Bear Stearns' predictions ...</td>\n",
       "      <td>In today's Daily Update you'll find free repor...</td>\n",
       "      <td>today daily update find free report america on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>allen-p\\all_documents\\10#1</td>\n",
       "      <td>Bloomberg Power Lines Report</td>\n",
       "      <td>Here is today's copy of Bloomberg Power Lines....</td>\n",
       "      <td>today copy bloomberg power line adobe acrobat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>allen-p\\all_documents\\100#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "      <td>initial set meet traders regard consolidate po...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>allen-p\\all_documents\\100#2</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>Phillip,\\n Below is the issues &amp; to do list as...</td>\n",
       "      <td>phillip issue list go forward document require...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>allen-p\\all_documents\\101#1</td>\n",
       "      <td>Consolidated positions: Issues &amp; To Do list</td>\n",
       "      <td>From our initial set of meetings with the trad...</td>\n",
       "      <td>initial set meet traders regard consolidate po...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     email_id  \\\n",
       "0           0    allen-p\\all_documents\\1#1   \n",
       "1           1   allen-p\\all_documents\\10#1   \n",
       "2           2  allen-p\\all_documents\\100#1   \n",
       "3           3  allen-p\\all_documents\\100#2   \n",
       "5           5  allen-p\\all_documents\\101#1   \n",
       "\n",
       "                                             subject  \\\n",
       "0  December 14, 2000 - Bear Stearns' predictions ...   \n",
       "1                       Bloomberg Power Lines Report   \n",
       "2        Consolidated positions: Issues & To Do list   \n",
       "3        Consolidated positions: Issues & To Do list   \n",
       "5        Consolidated positions: Issues & To Do list   \n",
       "\n",
       "                                               email  \\\n",
       "0  In today's Daily Update you'll find free repor...   \n",
       "1  Here is today's copy of Bloomberg Power Lines....   \n",
       "2  From our initial set of meetings with the trad...   \n",
       "3  Phillip,\\n Below is the issues & to do list as...   \n",
       "5  From our initial set of meetings with the trad...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  today daily update find free report america on...  \n",
       "1  today copy bloomberg power line adobe acrobat ...  \n",
       "2  initial set meet traders regard consolidate po...  \n",
       "3  phillip issue list go forward document require...  \n",
       "5  initial set meet traders regard consolidate po...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def lemmatize_list(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_words.append(lemmatizer.lemmatize(word, pos='v'))\n",
    "    return new_words\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    words = lemmatize_list(words)\n",
    "    return ' '.join(words)\n",
    "\n",
    "data['clean_text'] = data.apply(lambda row: normalize(row['clean_text']), axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "15fe8e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 0'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "09ef49a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_text']=data['clean_text'].apply(lambda x: x.replace('_',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0ae25352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty values for NA\n",
    "data['clean_text'].replace('', np.nan, inplace=True)\n",
    "# drop NAs\n",
    "data.dropna(subset=['clean_text'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "33f4657d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\split_threads_cleaned.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16eace90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\split_threads_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbe87e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09af8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(443173, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fd88549",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2= data.sample(n=20000)\n",
    "if type(data2['clean_text']) is list:\n",
    "    text = data2['clean_text']\n",
    "else:\n",
    "    text = data2['clean_text'].tolist()\n",
    "\n",
    "text=[str(x) for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3ad1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of GPU-accelerated UMAP and HDBSCAN\n",
    "# umap_model = UMAP(n_components=5, n_neighbors=15, min_dist=0.0)\n",
    "# hdbscan_model = HDBSCAN(min_samples=10, gen_min_span_tree=True)\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "#sentence_model = normalize(sentence_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baf15ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add this to remove stopwords\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 2), stop_words=\"english\")\n",
    "\n",
    "model = BERTopic(\n",
    "    vectorizer_model=vectorizer_model,\n",
    "    language='english', calculate_probabilities=True,\n",
    "    embedding_model=sentence_model,\n",
    "    verbose=True,\n",
    "#     umap_model=umap_model,\n",
    "#     hdbscan_model=hdbscan_model\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ea4547ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f772c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4fed5d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8df3cbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00953817367553711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 625,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500d11769bf04934a1c6f78f97a205c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-17 10:57:39,562 - BERTopic - Transformed documents to Embeddings\n",
      "2022-10-17 10:57:59,125 - BERTopic - Reduced dimensionality\n",
      "2022-10-17 10:58:55,495 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 30min 16s\n",
      "Wall time: 5min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device('/GPU:0'):\n",
    "    topics, probs = model.fit_transform(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc8f6bf",
   "metadata": {},
   "source": [
    "Execution times:\n",
    "* 5k examples: around 1 minute\n",
    "* 20k examples: around 5 minutes 13 s\n",
    "* 100k examples: around (started 5:09) 21 minutes the first two steps. Did not finish after 3 hours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af972f19",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd0551e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>10440</td>\n",
       "      <td>-1_thank_know_need_email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>533</td>\n",
       "      <td>0_weekend_night_good_hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>498</td>\n",
       "      <td>1_meet_pm_attend_conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>270</td>\n",
       "      <td>2_deal_kate_prebon_amerex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>185</td>\n",
       "      <td>3_fyi fyi_fyi_fyr fyi_fyi fyr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>177</td>\n",
       "      <td>4_enron north_enron_north america_america corp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>174</td>\n",
       "      <td>5_gas_thank_ena_gas market</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>128</td>\n",
       "      <td>6_jeff_best jeff_best_jeff jeff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>124</td>\n",
       "      <td>7_agreement_draft_letter_attach</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>123</td>\n",
       "      <td>8_nan_nan nan_nan jin_jin nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                            Name\n",
       "0     -1  10440                        -1_thank_know_need_email\n",
       "1      0    533                       0_weekend_night_good_hope\n",
       "2      1    498                     1_meet_pm_attend_conference\n",
       "3      2    270                       2_deal_kate_prebon_amerex\n",
       "4      3    185                   3_fyi fyi_fyi_fyr fyi_fyi fyr\n",
       "5      4    177  4_enron north_enron_north america_america corp\n",
       "6      5    174                      5_gas_thank_ena_gas market\n",
       "7      6    128                 6_jeff_best jeff_best_jeff jeff\n",
       "8      7    124                 7_agreement_draft_letter_attach\n",
       "9      8    123                   8_nan_nan nan_nan jin_jin nan"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = model.get_topic_info()\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7d235",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46091711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4ddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_hierarchy(hierarchical_topics= topic_model.hierarchical_topics(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d758e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74890bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "# Prepare embeddings\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = sentence_model.encode(text, show_progress_bar=False)\n",
    "\n",
    "# Train BERTopic\n",
    "topic_model = BERTopic().fit(text, embeddings)\n",
    "\n",
    "# Run the visualization with the original embeddings\n",
    "topic_model.visualize_documents(text, embeddings=embeddings)\n",
    "\n",
    "# Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "topic_model.visualize_documents(text, reduced_embeddings=reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedb73ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
