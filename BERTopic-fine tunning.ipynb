{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c64b2ae",
   "metadata": {},
   "source": [
    "# BERTopic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cf848c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\p_uli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\p_uli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\p_uli\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bertopic import BERTopic\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "import tensorflow as tf\n",
    "# from cuml.cluster import HDBSCAN\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.preprocessing import normalize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup   \n",
    "import contractions,unicodedata\n",
    "\n",
    "\n",
    "nltk.download('stopwords')                              # Download Stopwords.\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import PorterStemmer                     # Stemmer\n",
    "from nltk.corpus import stopwords                       # Import stopwords.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize  # Import Tokenizer.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer         # Import Lemmatizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16eace90",
   "metadata": {},
   "outputs": [],
   "source": [
    "data= pd.read_csv(r'C:\\Users\\p_uli\\Desktop\\Columbia University\\Cursos\\Fall 22\\Capstone\\Data\\split_threads_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd88549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data2= data.sample(n=20000)\n",
    "data2= data.sample(n=10000)#,random_state=42)\n",
    "if type(data2['clean_text']) is list:\n",
    "    text = data2['clean_text']\n",
    "else:\n",
    "    text = data2['clean_text'].tolist()\n",
    "\n",
    "text=[str(x) for x in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbe87e",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ad1c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the grid to implement Random Search CV\n",
    "\n",
    "## BERTopic hyperparameters\n",
    "# transformers. Taken from https://www.sbert.net/docs/pretrained_models.html\n",
    "s_transf=['all-MiniLM-L6-v2','all-MiniLM-L12-v2']\n",
    "\n",
    "# top n words per topic, it is recommended to keep this number between 10-20\n",
    "top_n_words=[int(x) for x in np.linspace(10,20,6)]\n",
    "\n",
    "# ngram range: Let's try 1,2 and 3-grams\n",
    "ngram_r=[(1,1),(1,2),(1,3)]\n",
    "\n",
    "# min topic size\n",
    "min_topic_s=[int(x) for x in np.linspace(10,200,6)]\n",
    "\n",
    "## UMAP hyperparameters\n",
    "# number of neighbors\n",
    "n_neigh=[int(x) for x in np.linspace(10,50,6)]\n",
    "\n",
    "## HDBSCAN hyperparameters\n",
    "# min cluster size\n",
    "min_cl_s=[int(x) for x in np.linspace(10,50,6)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8171cf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c7ef01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(transformer,topwords,n_gram,min_top, nn, min_cluster):\n",
    "    # embedding\n",
    "    sentence_model = SentenceTransformer(transformer)\n",
    "    \n",
    "    # ngrams\n",
    "    vectorizer_model = CountVectorizer(ngram_range=n_gram, stop_words=\"english\")\n",
    "    \n",
    "    # UMAP: dimensionality reduction\n",
    "    umap_model = UMAP(n_neighbors=nn, min_dist=0.0, metric='cosine', random_state=42)\n",
    "    \n",
    "    # HDBSCAN: clustering\n",
    "    hdbscan_model = HDBSCAN(min_cluster_size=min_cluster, metric='euclidean', \n",
    "                            cluster_selection_method='eom', prediction_data=True, min_samples=5)\n",
    "    \n",
    "    # BERTopic\n",
    "    model = BERTopic(\n",
    "        top_n_words=topwords,\n",
    "        min_topic_size=min_top,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        language='english', calculate_probabilities=True,\n",
    "        embedding_model=sentence_model,\n",
    "        verbose=True,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea1e17f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bertopic._bertopic.BERTopic at 0x1eae499a7f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=create_model(s_transf[0],top_n_words[0],ngram_r[0],min_topic_s[0],10,min_cl_s[0])\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8df3cbf2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016129732131958008,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Batches",
       "rate": null,
       "total": 313,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d3f5dae00d4ef8a9b85cd91c4ce719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-22 13:43:42,638 - BERTopic - Transformed documents to Embeddings\n",
      "2022-10-22 13:44:05,362 - BERTopic - Reduced dimensionality\n",
      "2022-10-22 13:44:37,275 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15min 35s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with tf.device('/GPU:0'):\n",
    "    topics, probs = model.fit_transform(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af972f19",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd0551e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>3459</td>\n",
       "      <td>-1_image_thank_know_email</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0_weekend_fun_good_mom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>1_clr_agreement_counterparty_credit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>102</td>\n",
       "      <td>2_dbcapsdata_database_error_alias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>3_game_fantasy_wr_update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>76</td>\n",
       "      <td>4_perlingiere_dperlin_debra_smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>76</td>\n",
       "      <td>5_vince_shall_shirley_thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>75</td>\n",
       "      <td>6_esmtp_receive_returnpath_smtp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>75</td>\n",
       "      <td>7_jeff_best_hee_talk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>74</td>\n",
       "      <td>8_enronxgate_acc_aps_industrials</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                 Name\n",
       "0     -1   3459            -1_image_thank_know_email\n",
       "1      0    172               0_weekend_fun_good_mom\n",
       "2      1    113  1_clr_agreement_counterparty_credit\n",
       "3      2    102    2_dbcapsdata_database_error_alias\n",
       "4      3     78             3_game_fantasy_wr_update\n",
       "5      4     76    4_perlingiere_dperlin_debra_smith\n",
       "6      5     76          5_vince_shall_shirley_thank\n",
       "7      6     75      6_esmtp_receive_returnpath_smtp\n",
       "8      7     75                 7_jeff_best_hee_talk\n",
       "9      8     74     8_enronxgate_acc_aps_industrials"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = model.get_topic_info()\n",
    "freq.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d7d235",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46091711",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2e4ddf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.visualize_hierarchy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d758e94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.visualize_barchart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74890bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Document visualization (left fot the best models)\n",
    "\n",
    "# from umap import UMAP\n",
    "# # Prepare embeddings\n",
    "\n",
    "# sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# embeddings = sentence_model.encode(text, show_progress_bar=False)\n",
    "\n",
    "# # Train BERTopic\n",
    "# topic_model = BERTopic().fit(text, embeddings)\n",
    "\n",
    "# # Run the visualization with the original embeddings\n",
    "# topic_model.visualize_documents(text, embeddings=embeddings)\n",
    "\n",
    "# # Reduce dimensionality of embeddings, this step is optional but much faster to perform iteratively:\n",
    "# reduced_embeddings = UMAP(n_neighbors=10, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "# topic_model.visualize_documents(text, reduced_embeddings=reduced_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a22a0e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
